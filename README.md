# Advancing movement neuroscience with foundation models

Quantifying movement and behaviour is essential across all of neuroscience, but traditional methods of collecting and analyzing movement data can be time-consuming, costly, and impractical in many real-world settings. This workshop introduces affordable, efficient strategies using video and pre-trained vision transformers as tools to gather contextually rich datasets, and movement foundation models as a way to combine them into joint insights. 

While the workshop will focus on human movement, the concepts will be broadly applicable across domains and species. Participants will gain hands-on experience with leading open-source tools, learning to extract detailed movement and contextual information from video using pre-trained models without the need for manual annotation. 

By the end of the workshop, attendees will not only have practical skills in using tools for movement analysis but also a deeper understanding of how to integrate this approach into broader research frameworks in neuroscience. Specific concepts covered will include pre-trained vision transformers for movement analysis; movement tokenization; movement foundation models; foundation model fine-tuning.

Much of the code is borrowed from: https://github.com/open-mmlab/mmpose/tree/main and https://github.com/CIS-522/course-content/tree/main
More documentation about MMPose (or the OpenMMLab suite of tools) can be found: https://openmmlab.com


|   | Run |
| - | --- | 
| Extracting Movment | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/quietscientist/Foundation_Models_Primer_MAIN2024EDU/blob/main/ExtractMovement.ipynb) | 
| Tokenizing Strategies | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/quietscientist/Foundation_Models_Primer_MAIN2024EDU/blob/main/TokenizingStrategies.ipynb) | 
