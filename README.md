# Advancing movement neuroscience with foundation models

Quantifying movement and behaviour is essential across all of neuroscience, but traditional methods of collecting and analyzing movement data can be time-consuming, costly, and impractical in many real-world settings. This workshop introduces affordable, efficient strategies using video and pre-trained vision transformers as tools to gather contextually rich datasets, and movement foundation models as a way to combine them into joint insights. 

While the workshop will focus on human movement, the concepts will be broadly applicable across domains and species. Participants will gain hands-on experience with leading open-source tools, learning to extract detailed movement and contextual information from video using pre-trained models without the need for manual annotation. 

By the end of the workshop, attendees will not only have practical skills in using tools for movement analysis but also a deeper understanding of how to integrate this approach into broader research frameworks in neuroscience. Specific concepts covered will include pre-trained vision transformers for movement analysis; movement tokenization; movement foundation models; foundation model fine-tuning.

